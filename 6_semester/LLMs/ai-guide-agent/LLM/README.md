In LM Studio don't forget to start your models
for llm: llama-3.2-3b-instruct or deepseek-r1-distill-qwen-7b
for embedding: text-embedding-all-minilm-l6-v2-embedding
change the model in configs.py (for both streamlit_app.py and app.py where the endpoint is computed) or main.py (mainly for streamlit)

üì¶ Create and Activate Virtual Environment
```bash
python3 -m venv venv #or
python3.11 -m llm-venv venv 

.\.venv\Scripts\Activate.ps1 #windows
source venv/bin/activate #linux / mac

```
 üìã Install Requirements
```bash
pip install -r LLM/requirements.txt
```

üöÄ Run the Streamlit App
```bash
streamlit run streamlit_app.py
# or 
streamlist run main.py
```

Start FastAPI Endpoint for LLM 
```bash
uvicorn app:app --reload
```




üåê Expose FastAPI to Your Network/Expo Go with ngrok

1. **Install ngrok:**
   - [Download from ngrok.com](https://ngrok.com/download) or use Homebrew:
     ```bash
     brew install ngrok # fro mac
     ```
     check the link for other os systems
   
2. **Sign Up to ngrok:**
    After you signed in, get your authentication token from the site

3 **Add your token on your local machine:**
    ngrok config add-authtoken <your_auth_token>

4. **Start ngrok to tunnel port 8000:**
   ```bash
   ngrok http 8000
   ```
   - You will see a forwarding URL like `https://abcd1234.ngrok.io`.

5. **Update your React Native app to use the ngrok URL:**
   - Place your new url generated by Ngrok at line 73 in PlacesScreen.tsx inside TravelTalk folder
   - Use `https://abcd1234.ngrok.io/api/llm_summary` as the API endpoint in your app.
   - Now any device (including Expo Go on your phone) can access your FastAPI backend.

---


